---
suppress-bibliography: true
---

```{r console_start, include=FALSE}
console_start()
```

```{console setup_history, include=FALSE}
 export CHAPTER="09"
 export HISTFILE=/history/history_${CHAPTER}
 rm -f $HISTFILE
```


# Modeling Data {#chapter-9-modeling-data}

In this chapter we’re going to perform the fourth and last step of the OSEMN model that we can do on a computer: modeling data.
Generally speaking, a model is an abstract or higher-level description of your data.
Just as with creating visualizations, it’s like taking a step back from the individual data points to see the bigger picture.

Visualizations, on the one hand, are characterized by shapes, positions, and colors such that we can interpret them by looking at them.
Models, on the other hand, are internally characterized by a bunch of numbers, which means that computers can use them, for example, to make predictions about a new data points.
(We can still visualize models so that we can try to understand them and see how they are performing.)

In this chapter I’ll consider three common types of algorithms to model data:

- Dimensionality reduction.
- Regression.
- Classification.

These algorithms come from the field of machine learning.
As such, I'm going to change the vocabulary a bit.
Let’s assume that I have a CSV file, also known as a *dataset*.
Each row, except for the header, is considered to be a *data point*.
Each data point has one or more *features*; properties that have been measured.
Sometimes, a data point also has a *label*; which is, generally speaking, a judgment.
This becomes more concrete when I introduce the wine dataset below.

The first type of algorithm (dimensionality reduction) is most often unsupervised, which means that they create a model based on the features of the dataset only.
The last two types of algorithms (regression and classification) are by definition supervised algorithms, which means that they also incorporate the labels into the model.

```{block2, type="rmdcaution"}
This is by no means an introduction to machine learning.
That implies that I must skim over many details.
My general advise is that you become familiar with an algorithm before applying it blindly to your data.
```

## Overview

In this chapter, you’ll learn how to:

- Reduce the dimensionality of your dataset using `tapkee` [@tapkee].
- Predict the quality of white wine using `vw` [@vw].
- Classify wine as red or white using `skll` [@skll].

This chapter starts with the following files:

```{console cd}
cd /data/ch09
l
```


## More Wine Please!

In this chapter, I’ll be using a dataset of wine tastings.
Specifically, red and white Portuguese "Vinho Verde" wine.
Each data point represents a wine, and consists of 11 physicochemical properties: (1) fixed acidity, (2) volatile acidity, (3) citric acid, (4) residual sugar, (5) chlorides, (6) free sulfur dioxide, (7) total sulfur dioxide, (8) density, (9) pH, (10) sulphates, and (11) alcohol.
There is also a quality score.
This score lies between 0 (very bad) and 10 (excellent) and is the median of at least three evaluation by wine experts. More information about this dataset is available at the [UCI Machine Learning Repository](http://archive.ics.uci.edu/ml/datasets/Wine+Quality).

The dataset is split into two files: one for white wine and one for red wine.
The very first step is to obtain the two files using `curl` (and of course `parallel` because I haven’t got all day):

```{console}
parallel "curl -sL http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-{}.csv > wine-{}.csv" ::: red white#!enter=FALSE
C-C#!literal=FALSE
```

The triple colon is just another way to pass data to `parallel`.

```{console cp_cache_wine}
cp /data/.cache/wine-*.csv .
```

Let’s inspect both files using and count the number of lines:

```{console, callouts=c(1, 2)}
< wine-red.csv nl |
fold |
trim
< wine-white.csv nl | fold | trim
wc -l wine-{red,white}.csv
```
<1> For clarity I use `nl`[@nl] to add line numbers
<2> To see the entire header, I use `fold`

At first sight this data appears to be quite clean.
Still, let’s scrub this data a little bit so that it conforms more with what most command-line tools are expecting.
Specifically, I’ll:

- Convert the header to lowercase.
- Replace the semi-colons with commas.
- Replace spaces with underscores.
- Remove unnecessary quotes.

These things can all be taken care of by `tr`.
Let’s use a for loop this time---for old times’ sake---to process both files:

```{console}
for COLOR in red white; do
< wine-$COLOR.csv tr '[A-Z]; ' '[a-z],_' | tr -d \" > wine-${COLOR}-clean.csv
done
```

Let’s also create a single dataset by combining the two files.
I’ll use `csvstack`[@csvstack] to add a column named *type* which will be "red" for rows of the first file, and "white" for rows of the second file:

```{console, callouts=2}
csvstack -g red,white -n type wine-{red,white}-clean.csv |
xsv select 2-,1 > wine.csv
```
<1> The new column *type* is placed at the beginning.
<2> Some algorithms assume that the label is the last column, so I use `xsv` to move the column *type* to the end.

It’s good practice to check whether there are any missing values in this dataset, because most machine learning algorithms can't handle them:

```{console}
csvstat wine.csv --nulls
```

Excellent! Just out of curiosity, let’s see what the how the distribution of quality looks like for both red and white wines.

```{console}
rush run -t 'ggplot(df, aes(x = quality, fill = type)) + geom_density(adjust = 3, alpha = 0.5)' wine.csv > wine-quality.png
display wine-quality.png
```
```{r plot_wine_quality, echo=FALSE, fig.cap="Comparing the quality of red and white wines using a density plot", fig.align="center", out.width="90%"}
knitr::include_graphics("images/wine-quality.png")
```

From the density plot you can see the quality of white wine is distributed more towards higher values.
Does this mean that white wines are overall better than red wines, or that the white wine experts more easily give higher scores than red wine experts?
That’s something that the data doesn't tell us.
Or is there perhaps a correlation between alcohol and quality?
Let’s use `rush` to find out:

```{console}
rush plot --x alcohol --y quality --color type --geom smooth wine.csv > wine-alcohol-vs-quality.png
display wine-alcohol-vs-quality.png
```
```{r plot_wine_alchohol_vs_quality, echo=FALSE, fig.cap="Correlation between the alcohol contents of wine and its quality", fig.align="center", out.width="90%"}
knitr::include_graphics("images/wine-alcohol-vs-quality.png")
```

Eureka! Ahem, let’s carry on with some modeling, shall we?


## Dimensionality Reduction with Tapkee

The goal of dimensionality reduction is to map high-dimensional data points onto a lower dimensional mapping.
The challenge is to keep similar data points close together on the lower-dimensional mapping.
As we’ve seen in the previous section, our wine dataset contains 13 features.
I’ll stick with two dimensions because that’s straight forward to visualize.

Dimensionality reduction is often regarded as being part of exploration.
It’s useful for when there are too many features for plotting.
You could do a scatter-plot matrix, but that only shows you two features at a time.
It’s also useful as a pre-processing step for other machine learning algorithms.

Most dimensionality reduction algorithms are unsupervised.
This means that they don’t employ the labels of the data points in order to construct the lower-dimensional mapping.

In this section I’ll look at two techniques: PCA, which stands for Principal Components Analysis [@Pearson1901] and t-SNE, which stands for t-distributed Stochastic Neighbor Embedding [@van2008visualizing].


### Introducing Tapkee

Tapkee is a C++ template library for dimensionality reduction [@Lisitsyn2013].
The library contains implementations of many dimensionality reduction algorithms, including:

- Locally Linear Embedding
- Isomap
- Multidimensional scaling
- PCA
- t-SNE

More information about these algorithms can be found on [Tapkee’s website](http://tapkee.lisitsyn.me/).
Although Tapkee is mainly a library that can be included in other applications, it also offers a command-line tool `tapkee`.
I’ll use this to perform dimensionality reduction on our wine dataset.


### Linear and Non-linear Mappings

First, I’ll scale the features using standardization such that each feature is equally important.
This generally leads to better results when applying machine learning algorithms.

To scale I use `rush` and the `tidyverse` package.

```{console, callouts=c(2:5)}
rush run --tidyverse --output wine-scaled.csv \
'select(df, -type) %>%
scale() %>%
as_tibble() %>%
mutate(type = df$type)' wine.csv
csvlook wine-scaled.csv
```
<1> I need to temporary remove the column *type* because `scale()` only works on numerical columns.
<2> The `scale()` function accepts a data frame, but returns a matrix.
<3> The function `as_tibble` converts the matrix back to a data frame.
<4> Finally, I add back the *type* column.

Now we apply both dimensionality reduction techniques and visualize the mapping using `Rio-scatter`:

```{console create_wine_pca}
xsv select '!type' wine-scaled.csv |
header -d |
tapkee --method pca |
tee wine-pca.txt | trim
```

<!-- TODO: MUST: Add some text in between these code snippets -->

```{console apply_pca}
< wine-pca.txt header -a pc1,pc2 |
paste -d, - <(xsv select type wine-scaled.csv) |
tee wine-pca.csv | csvlook
```


```{console plot_pca}
rush plot --x pc1 --y pc2 --color type wine-pca.csv > wine-pca.png
display wine-pca.png
```
```{r, echo=FALSE, fig.cap="Linear dimensionality reduction with PCA", fig.align="center", out.width="90%"}
knitr::include_graphics("images/wine-pca.png")
```

<!-- TODO: Fix cols util so that perhaps the next snippet can be improved -->
<!-- TODO: Annotate snippets -->

```{console apply_tsne}
xsv select '!type' wine-scaled.csv |
header -d |
tapkee --method t-sne |
header -a x,y |
paste -d, - <(xsv select type wine-scaled.csv) |
rush plot --x x --y y --color type > wine-tsne.png
display wine-tsne.png
```

```{r, echo=FALSE, fig.cap="Non-linear dimensionality reduction with t-SNE", fig.align="center", out.width="90%"}
knitr::include_graphics("images/wine-tsne.png")
```

<!-- TODO: MUST Improve this sentence: something with only custom tools -->
<!-- Note that there’s not a single *classic*  (i.e., classic command-line tool) in this one-liner. Now that’s the power of creating your own tools! -->


## Regression with Vowpal Wabbit


In this section, I’ll be predicting the quality of the white wine, based on their physicochemical properties.
Because the quality is a number between 0 and 10, we can consider predicting the quality as a regression task.

### Preparing the Data

Convert to `vw` format and split into 5 parts:

```{console}
csv2vw wine-white-clean.csv --label quality |
shuf |
split -d -n r/5 - wine-part-
wc -l wine-part-*
```

Combine parts into training and test datasets:

```{console}
mv wine-part-00 wine-test.vw
cat wine-part-* > wine-train.vw
rm wine-part-*
wc -l wine-*.vw
```

Let's have a look at the format:

```{console}
< wine-test.vw trim
```

### Train

Now we're ready to train a model using `vw`.
This tool accepts many different options (nearly 400!).
Luckily, you don't need all of them in order to be effective.
To explain the ones I use here properly, I’ll put each one on a separate line.

```{console}
vw \
--data wine-train.vw \
--final_regressor wine.model \
--passes 10 \
--cache_file wine.cache \
--nn 3 \
--quadratic :: \
--l2 0.000005 \
--bit_precision 25
```

### Test

The model, also known as the regressor, is stored in file *wine.model*.
To use that model to make predictions, I run `vw` again, but now with a different set of options:

```{console}
vw \
--data wine-test.vw \
--initial_regressor wine.model \
--testonly \
--predictions predictions \
--quiet
```

The predictions are stored in a file called *predictions*.
Let's combine those predictions with the true or observed values that are in *wine-test.vw* using `paste`.
Using `awk`, I can compare the predicted values with the observed values and compute the mean absolute error (MAE).
The MAE tells us how far off `vw` is on average, when it comes to predicting the quality of a white wine.

```{console}
paste -d, predictions <(cut -d '|' -f 1 wine-test.vw) |
tee results.csv |
awk -F, '{E+=sqrt(($1-$2)^2)} END {print "MAE: " E/NR}' |
cowsay
```

So, the predictions are on average about 0.6 points off.
Using `rush`, I visualize the correlation between the observed values and the predicted values:

```{console}
< results.csv header -a "predicted,observed" |
rush plot --x observed --y predicted --geom jitter > wine-regression.png
display wine-regression.png
```
```{r, echo=FALSE, fig.cap="Regression with Vowpal Wabbit", fig.align="center", out.width="90%"}
knitr::include_graphics("images/wine-regression.png")
```



## Classification with SciKit-Learn Laboratory

<!-- TODO: Explain SKLL better -->

In this section I'm going to classify wines as either red or wine.
I’ll be using SciKit-Learn Laboratory (SKLL) for this.
SKLL is a Python package that provides the `run_experiment` command-line tool.
I have aliased this to `skll` because I find it easier to remember as it corresponds to the package name:

```{console skll_alias}
alias skll=run_experiment
skll
```


### Preparing the Data


<!-- However, in this example, I'm going to use cross-validation, meaning that I only need to specify a training dataset. -->
<!-- Cross-validation is a technique that splits up the whole dataset into a certain number of subsets. These subsets are  called folds. (Usually, five or ten folds are used.) -->

<!-- TODO: Explain cross validation better, why is it used? -->
<!-- CV does the splitting for us. -->

`skll` expects that the training and test dataset have the same filenames, located in separate directories.
Because the predictions produced by `skll` are not necessarily in the same order as the original dataset, I add a column *id* that contains a unique identifier so that I can match the predictions and with the correct data points.
Let's create a balanced dataset:

```{console skll_create_features, callouts=c("NUM_RED", "csvstack", "nl", "sed")}
NUM_RED="$(< wine-red-clean.csv wc -l)"
csvstack -n type -g red,white \
wine-red-clean.csv \
<(< wine-white-clean.csv body shuf | head -n $NUM_RED) |
body shuf |
nl -s, -w1 -v0 |
sed '1s/0,/id,/' |
tee wine-balanced.csv | csvlook | trim
```
<1> Store the number of red wines in variable *NUM_RED*
<2> Combine all red wines with a random sample of white wines.
<3> Add "line numbers" using `nl` in front of each line
<4> Replace the "0" on the first line with "id" so that it's a proper column

Let's split this balanced dataset into a training set and a test set:

```{console}
mkdir -p {train,test}
HEADER="$(< wine-balanced.csv header)"
< wine-balanced.csv header -d | shuf | split -d -n r/5 - wine-part-
wc -l wine-part-*
cat wine-part-00 | header -a $HEADER > test/features.csv && rm wine-part-00
cat wine-part-* | header -a $HEADER > train/features.csv && rm wine-part-*
wc -l t*/features.csv
```


### Running the Experiment

<!-- TODO: Annotate configuration file -->

Create a configuration file called *predict-quality.cfg*:

```{console bat_cfg}
bat classify.cfg
```

I run the experiment using `skll`:

```{console skll}
skll -l classify.cfg 2>/dev/null
```

The option`-l` specifies to run in local mode.
`skll` also offers the possibility to run experiments on clusters.
The time it takes to run an experiment depends on the complexity of the chosen algorithms and the size of the data.


### Parsing the Results

Once all algorithms are done, the results can now be found in the directory *output*:

```{console ls_output}
ls -1 output
```

`skll` generates four files for each learner: one log, two with results, and one with predictions.

I  extract the algorithm names and sort them by their accuracies using the following SQL query:

```{console}
< output/wine_summary.tsv csvsql --query "SELECT learner_name, accuracy FROM stdin ORDER BY accuracy DESC" | csvlook -I
```

The relevant column here is *accuracy*, which indicates the percentage of data points that are classified correctly.
From this we see that actually all algorithms are performing really well.
The RandomForestClassifier comes out as best performing algorithm, closely followed by KNeighborsClassifier.

Each *json* file also contains a confusion matrix, giving you additional insight into the performance of each classifier.

```{console}
jq -r '.[] | "\(.learner_name):\n\(.result_table)\n"' output/*.json
```


## Further Reading

* Cortez, P., A. Cerdeira, F. Almeida, T. Matos, and J. Reis. 2009. “Modeling Wine Preferences by Data Mining from Physicochemical Properties.” <em>Decision Support Systems</em> 47 (4). Elsevier:547–53.
* Pearson, K. 1901. “On Lines and Planes of Closest Fit to Systems of Points in Space.” <em>Philosophical Magazine</em> 2 (11):559–72.
* Maaten, Laurens van der, and Geoffrey Everest Hinton. 2008. “Visualizing Data Using T-SNE.” <em>Journal of Machine Learning Research</em> 9:2579–2605.
